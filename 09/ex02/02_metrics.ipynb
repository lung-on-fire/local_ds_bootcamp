{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 09. Exercise 02\n",
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the same dataframe as in the previous exercise.\n",
    "2. Using `train_test_split` with parameters `test_size=0.2`, `random_state=21` get `X_train`, `y_train`, `X_test`, `y_test`. Use the additional parameter `stratify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1686, 44)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/day-of-week-not-scaled.csv')\n",
    "enrichment = pd.read_csv('../data/dayofweek.csv')\n",
    "df['dayofweek'] = enrichment['dayofweek']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('dayofweek', axis=1), df['dayofweek'], test_size=0.2, random_state=21, stratify=df['dayofweek'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the best parameters from the previous exercise and train the model of SVM.\n",
    "2. You need to calculate `accuracy`, `precision`, `recall`, `ROC AUC`.\n",
    "\n",
    " - `precision` and `recall` should be calculated for each class (use `average='weighted'`)\n",
    " - `ROC AUC` should be calculated for each class against any other class (all possible pairwise combinations) and then weighted average should be applied for the final metric\n",
    " - the code in the cell should display the result as below:\n",
    "\n",
    "```\n",
    "accuracy is 0.88757\n",
    "precision is 0.89267\n",
    "recall is 0.88757\n",
    "roc_auc is 0.97878\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calculation(model, X_train, X_test, y_train, y_test, **kwargs) -> None:\n",
    "    if kwargs:\n",
    "        model.set_params(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "    classes = model.classes_\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    # попарная классификация\n",
    "    roc_auc_scores = []\n",
    "    class_counts = [] \n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        for j in range(i + 1, n_classes):\n",
    "            # отбираем значения принадлежащие к одному из классов\n",
    "            mask = (y_test == classes[i]) | (y_test == classes[j])  #boolean array\n",
    "            y_true_ij = y_test[mask]\n",
    "            y_proba_ij = y_pred_proba[mask]\n",
    "            \n",
    "            # высчитываем бинарно\n",
    "            roc_ij = roc_auc_score(\n",
    "                (y_true_ij == classes[i]).astype(int),\n",
    "                y_proba_ij[:, i],  # вероятность класса i\n",
    "            )\n",
    "            roc_auc_scores.append(roc_ij)\n",
    "            \n",
    "            # считаем значения\n",
    "            count_ij = mask.sum()\n",
    "            class_counts.append(count_ij)\n",
    "    \n",
    "    # вычисляем средневзвешенное\n",
    "    weighted_roc_auc = np.average(roc_auc_scores, weights=class_counts)\n",
    "\n",
    "    acc = accuracy_score (y_test, y_pred)\n",
    "    prec = precision_score (y_test, y_pred, average='weighted')\n",
    "    rcl = recall_score (y_test, y_pred, average='weighted')\n",
    "\n",
    "    if acc:\n",
    "        print(f\"accuracy is {acc:.5f}\")\n",
    "        print(f\"precision is {prec:.5f}\")\n",
    "        print(f\"recall is {rcl:.5f}\")\n",
    "        print(f\"roc is {weighted_roc_auc:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_model = SVC(random_state=21, probability=True, C=10, class_weight=None, gamma='auto', kernel= 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.88757\n",
      "precision is 0.89267\n",
      "recall is 0.88757\n",
      "roc is 0.97965\n"
     ]
    }
   ],
   "source": [
    "metrics_calculation(SVM_model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The same task for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = DecisionTreeClassifier(random_state=21, max_depth=21, criterion='gini', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.88462\n",
      "precision is 0.88765\n",
      "recall is 0.88462\n",
      "roc is 0.93261\n"
     ]
    }
   ],
   "source": [
    "metrics_calculation(tree_model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The same task for random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestClassifier(random_state=21, n_estimators=100, max_depth=24, criterion='entropy', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.92604\n",
      "precision is 0.92754\n",
      "recall is 0.92604\n",
      "roc is 0.98762\n"
     ]
    }
   ],
   "source": [
    "metrics_calculation(forest_model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose the best model.\n",
    "2. Analyze: for which `weekday` your model makes the most errors (in % of the total number of samples of that class in your full dataset), for which `labname` and for which `users`.\n",
    "3. Save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1686 entries, 0 to 1685\n",
      "Data columns (total 44 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   numTrials         1686 non-null   int64  \n",
      " 1   hour              1686 non-null   int64  \n",
      " 2   uid_user_0        1686 non-null   float64\n",
      " 3   uid_user_1        1686 non-null   float64\n",
      " 4   uid_user_10       1686 non-null   float64\n",
      " 5   uid_user_11       1686 non-null   float64\n",
      " 6   uid_user_12       1686 non-null   float64\n",
      " 7   uid_user_13       1686 non-null   float64\n",
      " 8   uid_user_14       1686 non-null   float64\n",
      " 9   uid_user_15       1686 non-null   float64\n",
      " 10  uid_user_16       1686 non-null   float64\n",
      " 11  uid_user_17       1686 non-null   float64\n",
      " 12  uid_user_18       1686 non-null   float64\n",
      " 13  uid_user_19       1686 non-null   float64\n",
      " 14  uid_user_2        1686 non-null   float64\n",
      " 15  uid_user_20       1686 non-null   float64\n",
      " 16  uid_user_21       1686 non-null   float64\n",
      " 17  uid_user_22       1686 non-null   float64\n",
      " 18  uid_user_23       1686 non-null   float64\n",
      " 19  uid_user_24       1686 non-null   float64\n",
      " 20  uid_user_25       1686 non-null   float64\n",
      " 21  uid_user_26       1686 non-null   float64\n",
      " 22  uid_user_27       1686 non-null   float64\n",
      " 23  uid_user_28       1686 non-null   float64\n",
      " 24  uid_user_29       1686 non-null   float64\n",
      " 25  uid_user_3        1686 non-null   float64\n",
      " 26  uid_user_30       1686 non-null   float64\n",
      " 27  uid_user_31       1686 non-null   float64\n",
      " 28  uid_user_4        1686 non-null   float64\n",
      " 29  uid_user_6        1686 non-null   float64\n",
      " 30  uid_user_7        1686 non-null   float64\n",
      " 31  uid_user_8        1686 non-null   float64\n",
      " 32  labname_code_rvw  1686 non-null   float64\n",
      " 33  labname_lab02     1686 non-null   float64\n",
      " 34  labname_lab03     1686 non-null   float64\n",
      " 35  labname_lab03s    1686 non-null   float64\n",
      " 36  labname_lab05s    1686 non-null   float64\n",
      " 37  labname_laba04    1686 non-null   float64\n",
      " 38  labname_laba04s   1686 non-null   float64\n",
      " 39  labname_laba05    1686 non-null   float64\n",
      " 40  labname_laba06    1686 non-null   float64\n",
      " 41  labname_laba06s   1686 non-null   float64\n",
      " 42  labname_project1  1686 non-null   float64\n",
      " 43  dayofweek         1686 non-null   int64  \n",
      "dtypes: float64(41), int64(3)\n",
      "memory usage: 579.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    136\n",
       "1    274\n",
       "2    149\n",
       "3    396\n",
       "4    104\n",
       "5    271\n",
       "6    356\n",
       "Name: dayofweek, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = y_test.sum() # 1135\n",
    "day_counts = df['dayofweek'].value_counts().sort_index()\n",
    "day_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Rates by Weekday (%):\n",
      "true_label\n",
      "0    22.222222\n",
      "4    14.285714\n",
      "5     9.259259\n",
      "1     7.272727\n",
      "2     6.666667\n",
      "3     3.750000\n",
      "6     2.816901\n",
      "Name: is_error, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "forest_model.fit(X_train, y_train)\n",
    "y_pred = forest_model.predict(X_test)\n",
    "\n",
    "test_data = X_test.copy()\n",
    "test_data['true_label'] = y_test\n",
    "test_data['predicted_label'] = y_pred\n",
    "test_data['is_error'] = (y_test != y_pred).astype(int)\n",
    "#test_data\n",
    "error_rates_by_weekday = test_data.groupby('true_label')['is_error'].mean() * 100\n",
    "print(\"Error Rates by Weekday (%):\")\n",
    "print(error_rates_by_weekday.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_model_random_forest.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(forest_model, 'best_model_random_forest.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function that takes a list of different models and a corresponding list of parameters (dicts) and returns a dict that contains all the 4 metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(model_list, X_train, y_train, X_test, y_test, param_dict=None) -> dict:\n",
    "    results = {}\n",
    "    for model in model_list:\n",
    "        model_name = model.__class__.__name__\n",
    "        if param_dict:\n",
    "            model.set_params(**param_dict)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        metrics = {}\n",
    "\n",
    "        classes = model.classes_\n",
    "        n_classes = len(classes)\n",
    "\n",
    "        # попарная классификация\n",
    "        roc_auc_scores = []\n",
    "        class_counts = [] \n",
    "\n",
    "        for i in range(n_classes):\n",
    "            for j in range(i + 1, n_classes):\n",
    "                # отбираем значения принадлежащие к одному из классов\n",
    "                mask = (y_test == classes[i]) | (y_test == classes[j])  #boolean array\n",
    "                y_true_ij = y_test[mask]\n",
    "                y_proba_ij = y_pred_proba[mask]\n",
    "\n",
    "                # высчитываем бинарно\n",
    "                roc_ij = roc_auc_score(\n",
    "                    (y_true_ij == classes[i]).astype(int),\n",
    "                    y_proba_ij[:, i],  # вероятность класса i\n",
    "                )\n",
    "                roc_auc_scores.append(roc_ij)\n",
    "\n",
    "                # считаем значения\n",
    "                count_ij = mask.sum()\n",
    "                class_counts.append(count_ij)\n",
    "\n",
    "        # вычисляем средневзвешенное\n",
    "        weighted_roc_auc = np.average(roc_auc_scores, weights=class_counts)\n",
    "\n",
    "        acc = accuracy_score (y_test, y_pred)\n",
    "        prec = precision_score (y_test, y_pred, average='weighted', zero_division=0)\n",
    "        rcl = recall_score (y_test, y_pred, average='weighted')\n",
    "\n",
    "        if acc:\n",
    "            metrics['accuracy:'] = round(acc, 5)\n",
    "            metrics['precision:'] = round(prec, 5)\n",
    "            metrics ['recall:'] = round(rcl, 5)\n",
    "            metrics['roc_auc:'] = round(weighted_roc_auc,5)\n",
    "\n",
    "        results[model_name] = metrics\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    forest_model,\n",
    "    SVM_model\n",
    "]\n",
    "\n",
    "metrics = get_all_metrics(\n",
    "    model_list=models,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RandomForestClassifier': {'accuracy:': 0.92604,\n",
       "  'precision:': 0.92754,\n",
       "  'recall:': 0.92604,\n",
       "  'roc_auc:': 0.98762},\n",
       " 'SVC': {'accuracy:': 0.88757,\n",
       "  'precision:': 0.89267,\n",
       "  'recall:': 0.88757,\n",
       "  'roc_auc:': 0.97965}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shylaisa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
